<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Universal approximation theorem and Convex optimisation | We are the Champions</title><meta name="author" content="Penny Zhao"><meta name="copyright" content="Penny Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="🏂Machine learning application 开始观看 🥕 Universal approximation theorem ⛺什么是万能近似定理？ 万能近似定理的核心思想是：只要有足够多的神经元和适当的激活函数，一个具有单隐藏层的前馈神经网络就可以以任意精度逼近任何连续函数。 换句话说：神经网络的结构非常强大，可以用来模拟从输入到输出的各种复杂关系  ⛺公式   输入和输出空间：">
<meta property="og:type" content="article">
<meta property="og:title" content="Universal approximation theorem and Convex optimisation">
<meta property="og:url" content="https://pennyzhao1507288.github.io/posts/681561e5/index.html">
<meta property="og:site_name" content="We are the Champions">
<meta property="og:description" content="🏂Machine learning application 开始观看 🥕 Universal approximation theorem ⛺什么是万能近似定理？ 万能近似定理的核心思想是：只要有足够多的神经元和适当的激活函数，一个具有单隐藏层的前馈神经网络就可以以任意精度逼近任何连续函数。 换句话说：神经网络的结构非常强大，可以用来模拟从输入到输出的各种复杂关系  ⛺公式   输入和输出空间：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pennyzhao1507288.github.io/cover_image/36160_week9_cover.jpg">
<meta property="article:published_time" content="2024-11-23T00:38:51.000Z">
<meta property="article:modified_time" content="2024-11-23T02:00:48.921Z">
<meta property="article:author" content="Penny Zhao">
<meta property="article:tag" content="math">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pennyzhao1507288.github.io/cover_image/36160_week9_cover.jpg"><link rel="shortcut icon" href="/img/profile.jpg"><link rel="canonical" href="https://pennyzhao1507288.github.io/posts/681561e5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="giU_VQBe1Zf22KsD7tRopSheFkjqhH-YcrI8GOuM5Vo"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Universal approximation theorem and Convex optimisation',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/cursor.css"><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="/css/font.css"><meta name="google-site-verification" content="giU_VQBe1Zf22KsD7tRopSheFkjqhH-YcrI8GOuM5Vo" /><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(url(https://i.loli.net/2019/09/09/5oDRkWVKctx2b6A.png));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/profile.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/video/"><i class="fa-fw fas fa-video"></i><span> 视频集</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/cover_image/36160_week9_cover.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">We are the Champions</span></a><a class="nav-page-title" href="/"><span class="site-name">Universal approximation theorem and Convex optimisation</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/video/"><i class="fa-fw fas fa-video"></i><span> 视频集</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Universal approximation theorem and Convex optimisation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-23T00:38:51.000Z" title="发表于 2024-11-23 08:38:51">2024-11-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-23T02:00:48.921Z" title="更新于 2024-11-23 10:00:48">2024-11-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Mathematics-and-Applications-of-Machine-Learning/">Mathematics and Applications of Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>🏂<span class='p blue'>Machine learning application</span></h1>
<div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-gray"  style="width: 1%" aria-valuenow="1" aria-valuemin="0" aria-valuemax="100"><p>开始观看</p></div></div>
<h2 id="🥕-swig￼2">🥕 <emp>Universal approximation theorem</emp></h2>
<h3 id="⛺-swig￼3">⛺<wavy>什么是万能近似定理？</wavy></h3>
<p>万能近似定理的核心思想是：<strong>只要有足够多的神经元和适当的激活函数，一个具有单隐藏层的前馈神经网络就可以以任意精度逼近任何连续函数</strong>。</p>
<p><strong>换句话说：神经网络的结构非常强大，可以用来模拟从输入到输出的各种复杂关系</strong></p>
<hr>
<h3 id="⛺-swig￼4">⛺<wavy>公式</wavy></h3>
<ol>
<li>
<p><strong>输入和输出空间</strong>：</p>
<ul>
<li>输入空间 $\mathcal{X} \subseteq \mathbb{R}^M$：比如一个学生的学习成绩包含 $M$ 门课程的分数。</li>
<li>输出空间 $\mathcal{Y} \subseteq \mathbb{R}$：比如预测这个学生未来考试的总分。</li>
</ul>
</li>
<li>
<p><strong>神经网络的假设空间</strong>：<br>
$$<br>
\mathcal{M}_\varphi = \left{ h : h(x) = \langle w^{(2)}, \varphi(W^{(1)}x + b^{(1)}) \rangle \ \middle| \ p \in \mathbb{N}, W^{(1)} \in \mathbb{R}^{p \times M}, b^{(1)} \in \mathbb{R}^p, w^{(2)} \in \mathbb{R}^p \right}<br>
$$</p>
<ul>
<li>这表示神经网络通过一系列权重和偏置映射输入到输出。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="🪐-swig￼5">🪐<u>神经网络的步骤</u></h4>
<ol>
<li>
<p><strong>线性变换</strong>：</p>
<p>$W^{(1)} \cdot x + b^{(1)}$</p>
<ul>
<li>这是输入数据的线性组合。比如，$x$ 是学生的各科成绩，$W^{(1)}$ 是每科成绩的重要性权重，$b^{(1)}$ 是偏移量。</li>
</ul>
</li>
<li>
<p><strong>非线性激活</strong>：</p>
<p>$\varphi(W^{(1)}x + b^{(1)})$</p>
<ul>
<li>激活函数 $\varphi$ 用于引入非线性，使神经网络可以拟合复杂关系。常见的激活函数有 ReLU 和 Sigmoid。</li>
<li>实例：ReLU($z$) = $\max(0, z)$，可以理解为负分数清零，只有正分数起作用。</li>
</ul>
</li>
<li>
<p><strong>线性组合输出</strong>：</p>
<p>$\langle w^{(2)}, \cdot \rangle$</p>
<ul>
<li>输出层会将隐藏层的结果做线性组合，再得出最终结果（比如预测分数）。</li>
</ul>
</li>
</ol>
<hr>
<p>下图展示了一个典型的 <strong>多层感知机（MLP）</strong>，通过以下几步：</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week9_1.png" alt="Image" style="zoom:50%;" />
<ol>
<li>输入层：输入变量 $x_1, x_2, x_3$ 代表学生的三门成绩。</li>
<li>隐藏层：
<ul>
<li>权重矩阵 $W^{(1)}$ 和偏置 $b^{(1)}$ 对输入进行线性变换。</li>
<li>激活函数 $\varphi$ 对每个节点的输出进行非线性处理。</li>
</ul>
</li>
<li>输出层：
<ul>
<li>权重向量 $w^{(2)}$ 对隐藏层的输出加权求和，得出预测结果 $y$。</li>
</ul>
</li>
</ol>
<hr>
<p>:black_nib:实际例子</p>
<p><em>用学生成绩预测总成绩</em></p>
<p>假设我们想根据三门课程的成绩（输入 $x_1, x_2, x_3$）预测学生的总成绩（输出 $y$）。神经网络的过程可以这样理解：</p>
<ol>
<li><strong>输入层</strong>：输入 $x_1=80, x_2=75, x_3=85$。</li>
<li>隐藏层计算：
<ul>
<li>首先计算线性变换 $W^{(1)} \cdot x + b^{(1)}$。</li>
<li>假设 $W^{(1)} = \begin{bmatrix} 0.5 &amp; 0.2 &amp; 0.3 \ 0.1 &amp; 0.7 &amp; 0.2 \end{bmatrix}$ 和 $b^{(1)} = \begin{bmatrix} 0.1 \ 0.2 \end{bmatrix}$。</li>
<li>激活函数 ReLU 将负值清零。</li>
</ul>
</li>
<li>输出层计算：
<ul>
<li>结果经过 $w^{(2)}$ 的线性组合得到预测的 $y$。</li>
</ul>
</li>
</ol>
<p><font title="red">重要性</font>：如果没有激活函数（即 $\varphi$ 是恒等函数），神经网络只能表示简单的线性模型。加入激活函数后，网络能捕捉非线性关系，比如学生成绩中某些科目有更复杂的加权规则。</p>
<hr>
<h3 id="⛺-swig￼6">⛺<wavy>Theorem 9.1：万能近似定理</wavy></h3>
<ol>
<li>假设输入空间 $\mathcal{X} \subseteq \mathbb{R}^M$ 是compact（紧致的）的（即，输入空间是有界且闭合的）。</li>
<li>激活函数 $\varphi: \mathbb{R} \to \mathbb{R}$ 是连续的。</li>
</ol>
<p>那么，神经网络的假设空间 $\mathcal{M}_\varphi$ 在连续函数空间 $C(\mathcal{X})$ 中是dense(稠密)，<strong>当且仅当</strong> 激活函数 $\varphi$ 不是一个多项式。</p>
<hr>
<h4 id="🪐-swig￼7">🪐<u>紧致集（Compact Sets）</u></h4>
<p>紧致集 $\mathcal{X} \subseteq \mathbb{R}^M$ 是一个同时满足以下条件的集合：</p>
<ol>
<li><strong>有界（Bounded）</strong>：存在一个常数 $c &gt; 0$，使得集合中的所有点 $x$ 都满足 $|x| &lt; c$。</li>
<li><strong>闭（Closed）</strong>：集合包含它的边界点。例如，如果一个点列 ${b_n}$ 的极限是 $b_\infty$，那么 $b_\infty$ 必须也在集合中。</li>
</ol>
<hr>
<ul>
<li>有界：集合是有限范围的，不能无限扩展。</li>
<li>闭：集合包括所有的边界点，没有遗漏的“空洞”。</li>
</ul>
<ol>
<li><strong>例子</strong>：
<ul>
<li>闭区间 $[a, b]$ 是紧致的，例如 $[1, 3]$。</li>
<li>半径 $c$ 的闭球 ${x : |x| \leq c}$ 是紧致的。</li>
<li>有限点集合 ${x_1, x_2, \dots, x_N}$ 也是紧致的。</li>
</ul>
</li>
<li><strong>反例</strong>：
<ul>
<li>开区间 $(a, b)$ 不是紧致的，因为它不包含边界点 $a$ 和 $b$。</li>
<li>无界区间 $[a, \infty)$ 也不是紧致的，因为它没有边界。</li>
</ul>
</li>
</ol>
<p>:sunny:在神经网络训练中，我们通常假设输入数据 $\mathcal{X}$ 是紧致的，例如：</p>
<ul>
<li>图片像素值在 $[0, 255]$ 范围内。</li>
<li>学生考试分数在 $[0, 100]$ 范围内。</li>
</ul>
<p>紧致性保证了输入空间是有限的</p>
<hr>
<h3 id="⛺-swig￼8">⛺<wavy>多项式激活函数的限制</wavy></h3>
<h4 id="🪐-swig￼9">🪐<u>**为什么激活函数不能是多项式？**</u></h4>
<p>假设激活函数 $\varphi(x)$ 是一个 $n$ 次多项式：<br>
$$<br>
\varphi(x) = \sum_{j=0}^n \alpha_j x^j<br>
$$<br>
对于任意输入 $x$，我们神经网络的输出可以表示为：<br>
$$<br>
h(x) = \sum_{i=1}^p w_i^{(2)} \varphi \left( \sum_{m=1}^M W_{i,m}^{(1)} x_m + b_i^{(1)} \right)<br>
$$<br>
展开后，最终结果仍然是一个 $n$ 次多项式的组合，这意味着神经网络本质上仍然是一个多项式。</p>
<p>:mag:问题的根源</p>
<p>多项式的阶数是固定的（最大为 $n$），即使神经网络增加隐藏层宽度（增加 $p$），它的本质仍然是一个有限阶多项式。这会导致以下问题：</p>
<ol>
<li><strong>无法逼近非多项式函数</strong>：例如，正弦函数 $\sin(x)$ 或阶跃函数都无法通过有限阶多项式很好地拟合。</li>
<li><strong>过于局限</strong>：多项式对函数的拟合能力有限，不能表现出神经网络真正的灵活性。</li>
</ol>
<h4 id="🪐-swig￼10">🪐<u>**激活函数的选择**</u></h4>
<ul>
<li><strong>ReLU</strong>（线性整流单元）：$\varphi(x) = \max(0, x)$。</li>
<li><strong>Sigmoid</strong>：$\varphi(x) = \frac{1}{1+e^{-x}}$。</li>
</ul>
<hr>
<h3 id="⛺-swig￼11">⛺<wavy>万能近似定理的推广</wavy></h3>
<h4 id="🪐-swig￼12">🪐<u>不连续函数的逼近</u></h4>
<p>神经网络不仅可以逼近连续函数（$C(\mathcal{X})$），还可以逼近某些<strong>不连续函数</strong>，比如定义在 $L^2(\mathcal{X}, \mu_{\mathcal{X}}, \mathcal{Y})$ 空间中的函数。</p>
<ul>
<li><strong>$L^2$ 函数空间</strong>：包含了平方可积的函数，这些函数可能在某些点有跳跃或不连续，但整体上可以通过积分定义均方误差。</li>
<li>实际意义：比如阶跃函数（如 Heaviside 函数）在图像分割或分类中可能很重要，尽管它是不连续的。</li>
</ul>
<hr>
<h4 id="🪐-swig￼13">🪐<u>适用于非紧致空间的推广</u></h4>
<p>当输入空间 $\mathcal{X}$ 不紧致（比如 $\mathcal{X} = \mathbb{R}^M$）时，万能近似定理依然可以适用。这种情况下，我们需要改变对稠密性的度量方式，放宽对“全局一致性”的要求，改用局部测度（比如积分范数）。</p>
<ul>
<li>实际意义：神经网络常用来处理具有无限范围的输入（如时间序列中的时间步、自然数序列等），这些数据空间是非紧致的。</li>
</ul>
<hr>
<h3 id="⛺-swig￼14">⛺<wavy>示例：Example 1</wavy></h3>
<h4 id="🪐-swig￼15">🪐<u>关于激活函数的讨论</u></h4>
<p>例子中指出，大多数激活函数（如 ReLU、Sigmoid）满足[定理 9.1](#Theorem 9.1：万能近似定理)的假设，然而 <strong>Heaviside 函数</strong> 是一个特殊的例子：</p>
<ul>
<li><strong>Heaviside 函数</strong>：$H(x) = \begin{cases} 1, &amp; x \geq 0 \ 0, &amp; x &lt; 0 \end{cases}$  它是不连续的（在 $x=0$ 点有跳跃）。</li>
</ul>
<p>尽管 Heaviside 函数不满足连续性假设，它依然可以在 $L^2$ 空间中逼近一些目标函数，表明神经网络对离散或分类问题的潜力。</p>
<hr>
<h4 id="🪐-swig￼16">🪐<u>网络深度与神经元数量的比较</u></h4>
<p>下图展示了浅层神经网络和深层神经网络在不同深度下的<strong>渐近神经元数量</strong>关系：</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week9_2.png" alt="Image" style="zoom:60%;" />
<ol>
<li><strong>黑线（更多层，较少神经元）</strong>：随着层数增加，单层的神经元数量增长较慢。</li>
<li><strong>蓝线（较少层，更多神经元）</strong>：如果减少层数，则每层需要更大量的神经元才能达到相同的逼近效果。</li>
</ol>
<hr>
<p>:key:根据上图的结论我们得出：相比于增加单层神经元的数量，通过增加层数来达到逼近效果通常更高效，比如说在图像分类中，深层网络可以通过逐步提取低级到高级的特征，更易于拟合复杂模式。并且随着层数（$k$）增加，深层神经网络所需的总神经元数量增长较慢。</p>
<h3 id="⛺-swig￼17">⛺<wavy>深度对神经网络的重要性</wavy></h3>
<p>虽然万能近似定理说明了只有两层的神经网络（单隐藏层网络）可以逼近任何连续函数，但实际应用中，深度神经网络（更深的层数）更加高效且实用</p>
<h4 id="🪐-swig￼18">🪐<u>**深度与宽度的关系**</u></h4>
<ul>
<li>浅层神经网络（shallow neural network）：</li>
</ul>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week9_3.png" alt="Image" style="zoom:40%;" />
<blockquote>
<p><em><strong>浅层神经网络</strong></em>在输入层和输出层之间只有一个（或几个）隐藏层。输入层接收数据，隐藏层处理数据，最后一层产生输出。与深度神经网络相比，浅层神经网络更简单、更容易训练，计算效率更高，<em><strong>深度神经网络</strong></em>可能有数十层中的数千个隐藏单元。浅层网络通常用于较简单的任务，例如线性回归、二元分类或低维特征提取。</p>
</blockquote>
<pre><code>- 需要非常大的神经元数量（单层内的宽度）才能逼近复杂的函数。
- 对于某些函数，浅层网络的神经元数量可能以指数级（$\Omega(2^k)$）增长。
</code></pre>
<ul>
<li>深层神经网络（deep neural network）：</li>
</ul>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week9_4.png" alt="Image" style="zoom:43%;" />
<pre><code>- 增加网络的深度（层数）后，只需要较少的神经元（宽度）即可实现相同的逼近效果。
- 比如，对于深度为 $\Theta(k^3)$ 的网络，单层只需要 $\Theta(1)$ 的神经元就可以实现相同功能。
</code></pre>
<blockquote>
<ul>
<li>$O(\cdot)$：表示“至多是这个数量级”，即上界。</li>
<li>$\Omega(\cdot)$：表示“至少是这个数量级”，即下界。</li>
<li>$\Theta(\cdot)$：表示“正好是这个数量级”。</li>
</ul>
</blockquote>
<p>Telgarsky (2016) 证明了：</p>
<ul>
<li>深度网络更擅长表达复杂函数，浅层网络无法有效地表达这些函数。</li>
<li><strong>浅层网络在实践中效率低下</strong>，因为它需要更多的总神经元，导致内存和训练成本过高。</li>
</ul>
<hr>
<h4 id="🪐-swig￼19">🪐<u>复杂函数的表示</u></h4>
<p>深度网络的多层结构可以逐步提取特征，比如：</p>
<ol>
<li>浅层网络：
<ul>
<li>试图用一层直接表达复杂的函数关系，容易导致模型过宽（需要的神经元数量过多）。</li>
</ul>
</li>
<li>深层网络：
<ul>
<li>第一层提取低级特征（如边缘或局部信息）。</li>
<li>中间层组合低级特征形成更复杂的模式（如形状、轮廓）。</li>
<li>最后一层将模式映射到输出（如分类结果）。</li>
</ul>
</li>
</ol>
<h4 id="🪐-swig￼20">🪐<u>**内存和计算效率**</u></h4>
<p>浅层网络虽然在理论上可以逼近任意函数，但它的参数数量（即神经元数量）会随着问题复杂度爆炸性增长。这会导致：</p>
<ol>
<li><strong>内存需求高</strong>：存储这些参数需要巨大的计算资源。</li>
<li><strong>训练成本高</strong>：训练这些参数需要大量数据和时间。</li>
</ol>
<p>相比之下，深层网络通过引入更多的层次，可以以更少的参数实现相同或更优的逼近效果。</p>
<h2 id="🥕-swig￼21">🥕 <emp>Convex optimisation</emp></h2>
<p>优化的核心是通过调整模型参数 $w$ 来最小化经验误差，从而提高模型的性能。</p>
<h3 id="⛺-swig￼22">⛺<wavy>目标函数与优化问题的定义</wavy></h3>
<p>优化问题通常定义为：<br>
$$<br>
\min_{w \in \mathcal{W}} \frac{1}{N} \sum_{n=1}^N L(H(x_n, w), y_n)<br>
$$</p>
<ul>
<li><strong>$x_n$</strong>：输入数据点。</li>
<li><strong>$y_n$</strong>：数据对应的标签。</li>
<li><strong>$H(x_n, w)$</strong>：模型预测结果。</li>
<li><strong>$L(H(x_n, w), y_n)$</strong>：损失函数，衡量预测值和真实值之间的差距。</li>
<li><strong>$\mathcal{W}$</strong>：参数空间，表示 $w$ 可以取的所有可能值。</li>
</ul>
<p>:dart:目标</p>
<p>通过选择适当的参数 $w$ 来最小化目标函数，即损失的平均值。</p>
<hr>
<h3 id="⛺-swig￼23">⛺<wavy>基本概念</wavy></h3>
<h4 id="🪐-swig￼24">🪐<u>目标函数（Objective Function）</u></h4>
<p>定义为 $\Phi(w)$，描述了优化问题的目标。我们希望找到一个 $w^<em>$，使得：<br>
$$<br>
\Phi(w^</em>) \leq \Phi(w), \quad \forall w \in \mathcal{W}<br>
$$<br>
其中，$w^*$ 被称为目标函数的<strong>全局最优解（global minimiser）</strong>。</p>
<hr>
<h4 id="🪐-swig￼25">🪐<u>全局最优解与局部最优解</u></h4>
<ul>
<li><strong>全局最优解（Global Minimiser）</strong>：
<ul>
<li>全局最优解是在整个参数空间 $\mathcal{W}$ 内达到的最低点。</li>
<li>满足 $\Phi(w^*) \leq \Phi(w)$，对所有 $w \in \mathcal{W}$ 都成立。</li>
</ul>
</li>
<li><strong>局部最优解（Local Minimiser）</strong>：
<ul>
<li>局部最优解仅在一个局部区域（即 $w$ 的某个邻域内）是最低点。</li>
<li>存在一个 $\epsilon &gt; 0$，使得 $\Phi(w’) \leq \Phi(w)$，对所有 $|w - w’| &lt; \epsilon$ 都成立。</li>
</ul>
</li>
</ul>
<h4 id="🪐-swig￼26">🪐<u>**局部与全局的区别**</u></h4>
<ul>
<li>局部最优解不一定是全局最优解。例如，目标函数有多个“山谷”，每个山谷的最低点是局部最优解，但只有最深的山谷是全局最优解。</li>
</ul>
<hr>
<h4 id="🪐-swig￼27">🪐<u>开集与开放球</u></h4>
<p>在定义局部最优解时引入了“开集”和“开放球”的概念：</p>
<ol>
<li>开集（Open Set）：
<ul>
<li>一个集合 $A \subseteq \mathbb{R}^K$ 是开集，如果对任意 $x \in A$，存在一个 $\epsilon &gt; 0$，使得以 $x$ 为中心，半径为 $\epsilon$ 的开放球 $B^\circ(x, \epsilon)$ 完全包含在 $A$ 内。</li>
</ul>
</li>
<li>开放球（Open Ball）：
<ul>
<li>以 $x$ 为中心，$\epsilon$ 为半径的开放球定义为：$B^\circ(x, \epsilon) = {x’ \in \mathbb{R}^K : |x - x’| &lt; \epsilon}$</li>
</ul>
</li>
</ol>
<p>局部最优解在数学上可以被理解为：在某个开集内是全局最优解。</p>
<hr>
<h3 id="⛺-swig￼28">⛺<wavy>Theorem 9.2：局部最优解的梯度条件</wavy></h3>
<p>以上定理给出了局部最优解的一条必要条件：</p>
<p>如果 $w′∈W$ 是 $Φ$ 的局部最优解，那么 $∇Φ(w′)=0$</p>
<ul>
<li><strong>$\Phi$</strong>：目标函数。</li>
<li><strong>$\nabla \Phi(w’)$</strong>：目标函数 $\Phi$ 在 $w’$ 处的梯度（即偏导数向量）。</li>
<li>这个条件的意义是：在局部最优解点，目标函数的梯度必须为零。</li>
</ul>
<hr>
<span class='p red'>梯度为零的意义</span>
<ul>
<li><strong>梯度为零的直观解释</strong>：<br>
梯度 $\nabla \Phi(w)$ 指向目标函数增长最快的方向。在局部最优点，函数的变化率为零，因此梯度必须为零。</li>
<li>例子：
<ul>
<li>在一维情况下，局部最优点（极值点）是函数曲线的切线斜率为零的地方。例如，$f(x) = x^2$ 在 $x = 0$ 处达到最小值，其一阶导数 $f’(x) = 2x$ 在 $x = 0$ 处为零。</li>
</ul>
</li>
</ul>
<hr>
<span class='p red'>梯度为零是必要但不充分条件</span>
<ul>
<li>梯度 $\nabla \Phi(w) = 0$ 表示 $w$ 是一个候选点，可能是：
<ol>
<li><strong>局部最小值</strong>：函数在该点附近是最低的。</li>
<li><strong>局部最大值</strong>：函数在该点附近是最高的。</li>
<li><strong>鞍点</strong>：函数在某些方向上增加，在另一些方向上减少。</li>
</ol>
</li>
</ul>
<div class="note green icon-padding modern"><i class="note-icon fas fa-fan"></i><p>例子<br>
假设目标函数 $\Phi(w) = w^3$：</p>
<ul>
<li>一阶导数 $\Phi’(w) = 3w^2$。</li>
<li>在 $w = 0$ 时，梯度为零，但 $w = 0$ 是一个turning point，因为：
<ul>
<li>当 $w &gt; 0$ 时，$\Phi(w)$ 增加。</li>
<li>当 $w &lt; 0$ 时，$\Phi(w)$ 减少。</li>
</ul>
</li>
</ul>
</div>
<hr>
<h4 id="🪐-swig￼32">🪐<u>局部 vs. 全局最优</u></h4>
<ul>
<li>梯度是一个<strong>局部性质</strong>：它只反映了函数在某点附近的变化，而无法判断该点是否是全局最优点。</li>
<li><strong>解决方法</strong>：如果目标函数 $\Phi$ 是<strong>凸函数</strong>，那么局部最优解必定是全局最优解。</li>
</ul>
<h3 id="⛺-swig￼33">⛺<wavy>Theorem 9.3: 凸优化中的局部最优与全局最优</wavy></h3>
<p>(i) 如果目标函数是凸函数，局部最优解也是全局最优解</p>
<ul>
<li>假设 $\mathcal{W} \subseteq \mathbb{R}^K$ 是一个凸集合，$\Phi: \mathcal{W} \to \mathbb{R}$ 是一个凸函数。</li>
<li>那么：
<ul>
<li>如果 $w’ \in \mathcal{W}$ 是 $\Phi$ 的局部最优解（即 $w’$ 在局部区域内达到最小值），那么 $w’$ 也是 $\Phi$ 的全局最优解。</li>
</ul>
</li>
</ul>
<p>(ii) 梯度条件与全局最优</p>
<ul>
<li>如果 $\mathcal{W}$ 是开集，并且目标函数 $\Phi$ 是连续可微的（continuously differentiable），</li>
<li>那么，如果 $w’’ \in \mathcal{W}$ 满足 $\nabla \Phi(w’‘) = 0$，则 $w’'$ 是 $\Phi$ 的全局最优解。</li>
</ul>
<span class='p green'>定理 (ii) 梯度条件的意义</span>
<ul>
<li>如果目标函数 $\Phi$ 是凸函数，且在 $w’‘$ 处的梯度为零（$\nabla \Phi(w’‘) = 0$），那么该点 $w’'$ 就是全局最优解。</li>
<li>原因是：对于凸函数，梯度为零的点（即函数的平坦点）一定是最低点。</li>
</ul>
<hr>
<p>下图展示了一个典型函数 $\Phi(w)$ 的局部最优解和全局最优解：</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week9_5.png" alt="Image" style="zoom:50%;" />
<center style="color:blue;text-decoration:underline">We sketch a function Φ and illustrate its local and global minimisers.</center>
<ul>
<li>全局最优解（global minimiser）：在整个定义域 $\mathcal{W}$ 内，目标函数值最低的点。</li>
<li>局部最优解（local minimiser）：在某个局部区域内，目标函数值最低的点，但可能不是全局最优。</li>
</ul>
<h4 id="🪐-swig￼35">🪐<u>**凸函数的特殊性质**</u></h4>
<ul>
<li>凸函数的定义是：任意两点 $w_1, w_2 \in \mathcal{W}$ 和 $t \in [0,1]$，都满足： $\Phi(t w_1 + (1-t) w_2) \leq t \Phi(w_1) + (1-t) \Phi(w_2)$ 直观来说，凸函数的图像总是“向上弯曲”的，不会有多个局部最优解。</li>
<li>因此，凸函数的局部最优解必然是全局最优解。</li>
</ul>
<p>Proof:</p>
<div class="note info modern"><p>:key:证明思路</p>
<ul>
<li>假设 $w’$ 是局部最优解，但 $w’$ 不是全局最优解。</li>
<li>由于 $w’$ 不是全局最优解，存在一个点 $w^* \in \mathcal{W}$，使得 $\Phi(w^*) &lt; \Phi(w’)$。</li>
<li>利用凸性定义和构造中间点的方式，证明 $w’$ 不满足局部最优的条件，从而得到矛盾。</li>
</ul>
</div>
<hr>
<span class='p green'>(i) Proof:</span>
<ol>
<li>
<p>根据凸性的定义，对于任意 $\lambda \in [0, 1]$，有：</p>
<p>$$<br>
\Phi(\lambda w’ + (1-\lambda)w^<em>) \leq \lambda \Phi(w’) + (1-\lambda)\Phi(w^</em>).<br>
$$</p>
</li>
<li>
<p>因为假设 $\Phi(w^*) &lt; \Phi(w’)$，所以：</p>
<p>$$<br>
\Phi(\lambda w’ + (1-\lambda)w^*) &lt; \Phi(w’)<br>
$$</p>
</li>
<li>
<p>这说明：中间点 $\lambda w’ + (1-\lambda)w^*$ 的目标函数值小于 $w’$ 的值。</p>
</li>
<li>
<p>接下来，考虑 $w’$ 的局部最优性：</p>
<ul>
<li>对于局部最优解，应该在某个邻域内的所有点 $w$，满足 $\Phi(w’) \leq \Phi(w)$。</li>
<li>但通过上述构造的 $\lambda w’ + (1-\lambda)w^<em>$ 属于 $w’$ 的邻域（利用 $\epsilon$ 的开放球定义），并且 $\Phi(\lambda w’ + (1-\lambda)w^</em>) &lt; \Phi(w’)$，矛盾。</li>
</ul>
</li>
<li>
<p>因此，$w’$ 必须是全局最优解。</p>
</li>
</ol>
<span class='p green'>(ii) Proof:</span>
<div class="note info modern"><p>:key:证明思路</p>
<ul>
<li>利用凸性和目标函数的可微性。</li>
<li>梯度为零的点满足一阶最优条件，结合凸性的特性，证明该点是全局最优解。</li>
</ul>
</div>
<ol>
<li>
<p><strong>凸性的条件</strong>： 对于可微的凸函数 $\Phi$，任意 $w, \hat{w} \in \mathcal{W}$，有：</p>
<p>$$<br>
\Phi(w) \geq \Phi(\hat{w}) + \langle \nabla \Phi(\hat{w}), w - \hat{w} \rangle.<br>
$$</p>
</li>
<li>
<p>令 $\hat{w} = w’‘$ 且 $\nabla \Phi(w’') = 0$，代入上式：</p>
<p>$$<br>
\Phi(w) \geq \Phi(w’‘) + \langle 0, w - w’’ \rangle = \Phi(w’').<br>
$$</p>
</li>
<li>
<p>由此可知：</p>
<p>$$<br>
\Phi(w) \geq \Phi(w’'), \quad \forall w \in \mathcal{W}.<br>
$$</p>
<p>这正是 $w’'$ 是全局最优解的定义。</p>
</li>
</ol>
<h4 id="🪐-swig￼40">🪐<u>Remark: 凸优化在损失函数和模型中的应用</u></h4>
<h4 id="📁-swig￼41">📁<emp>凸优化与损失函数</emp></h4>
<p>许多损失函数和模型的组合会导致**经验误差（empirical error）**是关于参数 $w$ 的凸函数，这使得可以使用凸优化方法来解决问题。</p>
<ul>
<li><strong>例子：</strong>
<ul>
<li>线性回归：
<ul>
<li>损失函数：均方误差（Mean Squared Error, MSE）。</li>
<li>函数形式是关于参数 $w$ 的二次函数，是凸的。</li>
</ul>
</li>
<li>逻辑回归：
<ul>
<li>损失函数：交叉熵（Cross Entropy）。</li>
<li>函数形式是关于 $w$ 的凸函数。</li>
</ul>
</li>
</ul>
</li>
<li><strong>为什么重要？</strong>
<ul>
<li>对于这些凸问题，局部最优解就是全局最优解，因此可以通过梯度下降等方法高效找到最优解。</li>
</ul>
</li>
</ul>
<hr>
<h4 id="🪐-swig￼42">🪐<u>非凸优化问题</u></h4>
<ul>
<li>在深度神经网络的上下文中，经验误差通常是<strong>非凸的</strong>。
<ul>
<li>损失函数的复杂性和神经网络的多层结构会导致多个局部最优点和鞍点。</li>
<li>非凸问题的挑战：
<ul>
<li>很难保证找到的是全局最优解。</li>
<li>需要使用诸如随机梯度下降（SGD）等启发式方法找到足够好的解。</li>
</ul>
</li>
</ul>
</li>
<li><strong>例子：</strong>
<ul>
<li>神经网络的交叉熵损失结合复杂的多层参数模型，通常会引入非凸性。</li>
<li>深度学习中的优化方法需要处理高维空间中的非凸问题。</li>
</ul>
</li>
</ul>
<p>以上文档的pdf可以用以下链接下载：<br>
<a target="_blank" rel="noopener" href="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/36160_week9.pdf" class="download-button" download><br>
<i class="fas fa-download"></i> 下载文档<br>
</a></p>
<div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-red"  style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100"><p>观看结束，感谢观看</p></div></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://pennyzhao1507288.github.io">Penny Zhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://pennyzhao1507288.github.io/posts/681561e5/">https://pennyzhao1507288.github.io/posts/681561e5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://pennyzhao1507288.github.io" target="_blank">We are the Champions</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/math/">math</a><a class="post-meta__tags" href="/tags/machine-learning/">machine learning</a></div><div class="post-share"><div class="social-share" data-image="/cover_image/36160_week9_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/posts/5038e228/" title="measure therory-expectation and limits in prob space"><img class="cover" src="/cover_image/37021_week8_cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">measure therory-expectation and limits in prob space</div></div><div class="info-2"><div class="info-item-1"> 🏂Modern Prob 开始观看  🥕 Expectation  ⛺什么是期望？ 首先，我们假设有一个随机变量 XXX，它是在概率空间 (Ω,F,P)(\Omega, \mathcal{F}, \mathbb{P})(Ω,F,P) 上定义的。简单来说，这意味着 XXX 在某种“世界”中随机地取值，而我们希望了解它的“平均值”或“期望”，也就是 E(X)\mathbb{E}(X)E(X)。在这个世界中，Ω\OmegaΩ 是所有可能发生的情况的集合（称为样本空间），F\mathcal{F}F 是这些事件的集合，P\mathbb{P}P 是每个事件发生的概率。 我们可以将期望 E(X)\mathbb{E}(X)E(X) 想象成这样一个问题：假设你每次从一个袋子中随机取一个数 XXX，那么你取到的数的平均值会是多少呢？这就是我们要用 Lebesgue-Stieltjes 积分来计算的。 公式： E(X):=∫ΩX dP\mathbb{E}(X) := \int_{\Omega} X \,...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/1deaf85c/" title="bayes hypothesis and artificial neural network"><img class="cover" src="/cover_image/36160_week8_cover.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-11</div><div class="info-item-2">bayes hypothesis and artificial neural network</div></div><div class="info-2"><div class="info-item-1"> 🏂Machine learning application 开始观看  🥕 Bayes hypothesis and Bayes noise 在机器学习中，监督学习的目标是从输入（也称为特征空间，记为X\mathcal{X}X）到输出（或标签空间，记为Y\mathcal{Y}Y）中找到一个映射关系，使得我们的预测与实际数据尽可能接近。我们假设数据是从某个分布 μZ\mu_ZμZ​ 中生成的，并且我们使用一个损失函数 LLL 来衡量预测与真实值之间的误差。  ⛺Bayes Hypothesis  🪐定义 为了找到最优的预测，我们需要找到一个能够最小化泛化误差 RRR 的函数（或模型）。这种“最优的”模型在这里称为 Bayes Hypothesis，记作 hBayesh^{\text{Bayes}}hBayes。它的定义如下： hBayes∈arg⁡min⁡h:X→Y∫ZL(y,h(x))dμZ(x,y).h^{\text{Bayes}} \in \arg \min_{h : \mathcal{X} \rightarrow \mathcal{Y}} \int_Z L(y,...</div></div></div></a><a class="pagination-related" href="/posts/223d6b0/" title="EM algorithm and Bayes classifier"><img class="cover" src="/cover_image/38161_week8_cover.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-11</div><div class="info-item-2">EM algorithm and Bayes classifier</div></div><div class="info-2"><div class="info-item-1"> 🏂Multivariate Statistics and Machine Learning 开始观看  🥕 EM算法 先来回顾一下log-likelihood functions for a GMM:  ⛺完全数据的对数似然函数（Complete Data Log-Likelihood） 首先，假设我们有数据 X\mathbf{X}X 和隐藏的变量 y\mathbf{y}y，则完全数据的对数似然函数定义为： ℓ(θ∣X,y)=∑i=1nlog⁡f(xi,yi∣θ)=∑i=1nlog⁡πyiNd(xi∣μyi,Σyi)\ell(\theta|\mathbf{X}, \mathbf{y}) = \sum_{i=1}^{n} \log f(x_i, y_i|\theta) = \sum_{i=1}^{n} \log \pi_{y_i} N_d(x_i|\mu_{y_i}, \Sigma_{y_i}) ℓ(θ∣X,y)=i=1∑n​logf(xi​,yi​∣θ)=i=1∑n​logπyi​​Nd​(xi​∣μyi​​,Σyi​​) 这个公式表示我们在已知每个数据点...</div></div></div></a><a class="pagination-related" href="/posts/5038e228/" title="measure therory-expectation and limits in prob space"><img class="cover" src="/cover_image/37021_week8_cover.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-15</div><div class="info-item-2">measure therory-expectation and limits in prob space</div></div><div class="info-2"><div class="info-item-1"> 🏂Modern Prob 开始观看  🥕 Expectation  ⛺什么是期望？ 首先，我们假设有一个随机变量 XXX，它是在概率空间 (Ω,F,P)(\Omega, \mathcal{F}, \mathbb{P})(Ω,F,P) 上定义的。简单来说，这意味着 XXX 在某种“世界”中随机地取值，而我们希望了解它的“平均值”或“期望”，也就是 E(X)\mathbb{E}(X)E(X)。在这个世界中，Ω\OmegaΩ 是所有可能发生的情况的集合（称为样本空间），F\mathcal{F}F 是这些事件的集合，P\mathbb{P}P 是每个事件发生的概率。 我们可以将期望 E(X)\mathbb{E}(X)E(X) 想象成这样一个问题：假设你每次从一个袋子中随机取一个数 XXX，那么你取到的数的平均值会是多少呢？这就是我们要用 Lebesgue-Stieltjes 积分来计算的。 公式： E(X):=∫ΩX dP\mathbb{E}(X) := \int_{\Omega} X \,...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/profile.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Penny Zhao</div><div class="author-info-description">愿世间美好与你环环相扣</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/m0_69003698" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="mailto:zhaopeiyu150728899@163.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">你好，有需要可以关注公众号：画皮述说馆</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">🏂Machine learning application</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%A5%95-swig%EF%BF%BC2"><span class="toc-number">1.1.</span> <span class="toc-text">🥕 Universal approximation theorem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC3"><span class="toc-number">1.1.1.</span> <span class="toc-text">⛺什么是万能近似定理？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC4"><span class="toc-number">1.1.2.</span> <span class="toc-text">⛺公式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC5"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">🪐神经网络的步骤</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC6"><span class="toc-number">1.1.3.</span> <span class="toc-text">⛺Theorem 9.1：万能近似定理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC7"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">🪐紧致集（Compact Sets）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC8"><span class="toc-number">1.1.4.</span> <span class="toc-text">⛺多项式激活函数的限制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC9"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">🪐**为什么激活函数不能是多项式？**</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC10"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">🪐**激活函数的选择**</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC11"><span class="toc-number">1.1.5.</span> <span class="toc-text">⛺万能近似定理的推广</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC12"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">🪐不连续函数的逼近</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC13"><span class="toc-number">1.1.5.2.</span> <span class="toc-text">🪐适用于非紧致空间的推广</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC14"><span class="toc-number">1.1.6.</span> <span class="toc-text">⛺示例：Example 1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC15"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">🪐关于激活函数的讨论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC16"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">🪐网络深度与神经元数量的比较</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC17"><span class="toc-number">1.1.7.</span> <span class="toc-text">⛺深度对神经网络的重要性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC18"><span class="toc-number">1.1.7.1.</span> <span class="toc-text">🪐**深度与宽度的关系**</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC19"><span class="toc-number">1.1.7.2.</span> <span class="toc-text">🪐复杂函数的表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC20"><span class="toc-number">1.1.7.3.</span> <span class="toc-text">🪐**内存和计算效率**</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%A5%95-swig%EF%BF%BC21"><span class="toc-number">1.2.</span> <span class="toc-text">🥕 Convex optimisation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC22"><span class="toc-number">1.2.1.</span> <span class="toc-text">⛺目标函数与优化问题的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC23"><span class="toc-number">1.2.2.</span> <span class="toc-text">⛺基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC24"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">🪐目标函数（Objective Function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC25"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">🪐全局最优解与局部最优解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC26"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">🪐**局部与全局的区别**</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC27"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">🪐开集与开放球</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC28"><span class="toc-number">1.2.3.</span> <span class="toc-text">⛺Theorem 9.2：局部最优解的梯度条件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC32"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">🪐局部 vs. 全局最优</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC33"><span class="toc-number">1.2.4.</span> <span class="toc-text">⛺Theorem 9.3: 凸优化中的局部最优与全局最优</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC35"><span class="toc-number">1.2.4.1.</span> <span class="toc-text">🪐**凸函数的特殊性质**</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC40"><span class="toc-number">1.2.4.2.</span> <span class="toc-text">🪐Remark: 凸优化在损失函数和模型中的应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%93%81-swig%EF%BF%BC41"><span class="toc-number">1.2.4.3.</span> <span class="toc-text">📁凸优化与损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC42"><span class="toc-number">1.2.4.4.</span> <span class="toc-text">🪐非凸优化问题</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/681561e5/" title="Universal approximation theorem and Convex optimisation"><img src="/cover_image/36160_week9_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Universal approximation theorem and Convex optimisation"/></a><div class="content"><a class="title" href="/posts/681561e5/" title="Universal approximation theorem and Convex optimisation">Universal approximation theorem and Convex optimisation</a><time datetime="2024-11-23T00:38:51.000Z" title="发表于 2024-11-23 08:38:51">2024-11-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/5038e228/" title="measure therory-expectation and limits in prob space"><img src="/cover_image/37021_week8_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="measure therory-expectation and limits in prob space"/></a><div class="content"><a class="title" href="/posts/5038e228/" title="measure therory-expectation and limits in prob space">measure therory-expectation and limits in prob space</a><time datetime="2024-11-14T18:05:12.000Z" title="发表于 2024-11-15 02:05:12">2024-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/223d6b0/" title="EM algorithm and Bayes classifier"><img src="/cover_image/38161_week8_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="EM algorithm and Bayes classifier"/></a><div class="content"><a class="title" href="/posts/223d6b0/" title="EM algorithm and Bayes classifier">EM algorithm and Bayes classifier</a><time datetime="2024-11-11T03:15:30.000Z" title="发表于 2024-11-11 11:15:30">2024-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/1deaf85c/" title="bayes hypothesis and artificial neural network"><img src="/cover_image/36160_week8_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="bayes hypothesis and artificial neural network"/></a><div class="content"><a class="title" href="/posts/1deaf85c/" title="bayes hypothesis and artificial neural network">bayes hypothesis and artificial neural network</a><time datetime="2024-11-10T23:14:28.000Z" title="发表于 2024-11-11 07:14:28">2024-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/67e70231/" title="Introduction to AI-Visual Odometry"><img src="/AI/week7_cover2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Introduction to AI-Visual Odometry"/></a><div class="content"><a class="title" href="/posts/67e70231/" title="Introduction to AI-Visual Odometry">Introduction to AI-Visual Odometry</a><time datetime="2024-11-10T18:12:21.000Z" title="发表于 2024-11-11 02:12:21">2024-11-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Penny Zhao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'pennyzhao1507288/pennyzhao1507288.github.io',
      'data-repo-id': 'R_kgDONMjCuQ',
      'data-category-id': 'DIC_kwDONMjCuc4CkIEb',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !true) {
    if (true) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>