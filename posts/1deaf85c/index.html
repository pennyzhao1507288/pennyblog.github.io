<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>bayes hypothesis and artificial neural network | We are the Champions</title><meta name="author" content="Penny Zhao"><meta name="copyright" content="Penny Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="🏂Machine learning application 开始观看 🥕 Bayes hypothesis and Bayes noise 在机器学习中，监督学习的目标是从输入（也称为特征空间，记为$\mathcal{X}$）到输出（或标签空间，记为$\mathcal{Y}$）中找到一个映射关系，使得我们的预测与实际数据尽可能接近。我们假设数据是从某个分布 $\mu_Z$ 中生成的，并且我们使">
<meta property="og:type" content="article">
<meta property="og:title" content="bayes hypothesis and artificial neural network">
<meta property="og:url" content="https://pennyzhao1507288.github.io/posts/1deaf85c/index.html">
<meta property="og:site_name" content="We are the Champions">
<meta property="og:description" content="🏂Machine learning application 开始观看 🥕 Bayes hypothesis and Bayes noise 在机器学习中，监督学习的目标是从输入（也称为特征空间，记为$\mathcal{X}$）到输出（或标签空间，记为$\mathcal{Y}$）中找到一个映射关系，使得我们的预测与实际数据尽可能接近。我们假设数据是从某个分布 $\mu_Z$ 中生成的，并且我们使">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pennyzhao1507288.github.io/cover_image/36160_week8_cover.jpg">
<meta property="article:published_time" content="2024-11-10T23:14:28.000Z">
<meta property="article:modified_time" content="2024-11-11T00:24:21.523Z">
<meta property="article:author" content="Penny Zhao">
<meta property="article:tag" content="math">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pennyzhao1507288.github.io/cover_image/36160_week8_cover.jpg"><link rel="shortcut icon" href="/img/profile.jpg"><link rel="canonical" href="https://pennyzhao1507288.github.io/posts/1deaf85c/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'bayes hypothesis and artificial neural network',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/cursor.css"><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="/css/font.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load', preloader.endLoading)

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(url(https://i.loli.net/2019/09/09/5oDRkWVKctx2b6A.png));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/profile.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/cover_image/36160_week8_cover.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">We are the Champions</span></a><a class="nav-page-title" href="/"><span class="site-name">bayes hypothesis and artificial neural network</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">bayes hypothesis and artificial neural network</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-10T23:14:28.000Z" title="发表于 2024-11-11 07:14:28">2024-11-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-11-11T00:24:21.523Z" title="更新于 2024-11-11 08:24:21">2024-11-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Mathematics-and-Applications-of-Machine-Learning/">Mathematics and Applications of Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>🏂<span class='p blue'>Machine learning application</span></h1>
<div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-gray"  style="width: 1%" aria-valuenow="1" aria-valuemin="0" aria-valuemax="100"><p>开始观看</p></div></div>
<h2 id="🥕-swig￼2">🥕 <emp>Bayes hypothesis and Bayes noise</emp></h2>
<p>在机器学习中，<strong>监督学习</strong>的目标是从输入（也称为特征空间，记为$\mathcal{X}$）到输出（或标签空间，记为$\mathcal{Y}$）中找到一个映射关系，使得我们的预测与实际数据尽可能接近。我们假设数据是从某个<strong>分布</strong> $\mu_Z$ 中生成的，并且我们使用一个<strong>损失函数</strong> $L$ 来衡量预测与真实值之间的误差。</p>
<h3 id="⛺-swig￼3">⛺<wavy>Bayes Hypothesis</wavy></h3>
<h4 id="🪐-swig￼4">🪐<u>定义</u></h4>
<p>为了找到最优的预测，我们需要找到一个能够最小化泛化误差 $R$ 的函数（或模型）。这种“最优的”模型在这里称为 <strong>Bayes Hypothesis</strong>，记作 $h^{\text{Bayes}}$。它的定义如下：</p>
<p>$$<br>
h^{\text{Bayes}} \in \arg \min_{h : \mathcal{X} \rightarrow \mathcal{Y}} \int_Z L(y, h(x)) d \mu_Z (x, y).<br>
$$</p>
<p>在这个公式中：</p>
<ul>
<li>$\arg \min_{h : \mathcal{X} \rightarrow \mathcal{Y}}$ 表示我们在所有从输入空间 $\mathcal{X}$ 到输出空间 $\mathcal{Y}$ 的函数中寻找最小化泛化误差的那个函数。</li>
<li>$\int_Z L(y, h(x)) d \mu_Z (x, y)$ 是一个积分表达式，表示在所有可能的数据 $(x, y)$ 上，对损失函数 $L(y, h(x))$ 的加权平均（权重由数据分布 $\mu_Z$ 确定）。</li>
<li>损失函数 $L(y, h(x))$ 表示真实值 $y$ 与预测值 $h(x)$ 之间的误差。</li>
</ul>
<h4 id="🪐-swig￼5">🪐<u>Bayes Hypothesis的作用</u></h4>
<p>Bayes Hypothesis可以被看作是一个“理论上的最优预测器”，即在给定的损失函数 $L$ 下，它是<strong>无法被进一步改进的</strong>。换句话说，它为我们提供了一个基准，我们的任何模型都试图去接近这个最优值。</p>
<p>在监督学习中，我们通过模型训练，隐式地在逼近这个Bayes Hypothesis，尽管我们可能永远无法达到它。</p>
<h4 id="🪐-swig￼6">🪐<u>特定情境下的Bayes Hypothesis</u></h4>
<p>在一些特殊情况下，Bayes Hypothesis是可以具体表达出来的。例如，在 <strong>平方损失（quadratic loss）</strong> 下，Bayes Hypothesis可以写作：</p>
<p>$$<br>
h^{\text{Bayes}}(x) = \mathbb{E}[Y | X = x],<br>
$$</p>
<p>即在给定输入 $X = x$ 的条件下，对应输出 $Y$ 的条件期望。这意味着在平方损失的情况下，最优的预测就是每个输入点上输出的平均值。这也是统计中最优预测器的一个经典结果。</p>
<p>对于 <strong>零一损失（zero-one loss）</strong>，也可以推导出对应的Bayes Hypothesis。</p>
<p>好的，Penny，让我们详细地解释一下这个定理和它的证明过程。</p>
<h3 id="⛺-swig￼7">⛺<wavy>定理 1</wavy></h3>
<p><strong>问题背景</strong>：我们考虑一个二分类问题，其中输入空间为$\mathcal{X} \subset \mathbb{R}^M$，标签空间为$\mathcal{Y} = {-1, 1}$。在这个设定下，损失函数是 <strong>零一损失</strong>（zero-one loss），表示为 $L(y, y’) = 1[y \neq y’]$，意思是当预测值和真实值不同的时候损失为1，否则为0。假设$(X, Y) \sim \mu_Z$，其中$X$和$Y$的联合分布为$\mu_Z$。</p>
<p><strong>结论</strong>：Bayes最优预测函数 $h^{\text{Bayes}}(x)$ 在这种情况下可以表示为：<br>
$$<br>
h^{\text{Bayes}}(x) = 2 \cdot 1[\mathbb{P}(Y = 1|X = x) &gt; 1/2] - 1<br>
$$</p>
<p>或等价地写为：<br>
$$<br>
h^{\text{Bayes}}(x) =<br>
\begin{cases}<br>
1, &amp; \text{如果 } \mathbb{P}(Y = 1|X = x) &gt; 1/2, \<br>
-1, &amp; \text{如果 } \mathbb{P}(Y = -1|X = x) \geq 1/2.<br>
\end{cases}<br>
$$</p>
<p>这表明：在二分类问题中，最优预测总是选择条件概率较大的那个标签。比如，如果在给定$x$的条件下，$Y = 1$的概率超过$1/2$，那么我们预测$Y = 1$，否则预测$Y = -1$。</p>
<p><strong>解释</strong></p>
<p>该定理说明了在二分类问题中，最优模型应选择具有更大条件概率的标签。这类似于投票中的“多数胜出”原则。即在给定输入$x$时，哪个类别的条件概率更高，我们就预测为哪个类别。</p>
<span class='p red'>证明思路</span>
<p><strong>1. 泛化误差的定义</strong></p>
<p>对于任何预测函数$h : \mathcal{X} \rightarrow \mathcal{Y}$，它的泛化误差$R(h)$可以写成：</p>
<p>$$<br>
\int_Z L(y, h(x)) d\mu_Z(x, y) = \int_{\mathcal{X}} \sum_{y \in \mathcal{Y}} 1[y \neq h(x)] \pi_{Y|X=x}(y) \pi_{\mathcal{X}}(x) dx,<br>
$$</p>
<p>其中$\pi_{Y|X=x}(y)$表示在给定$X=x$的条件下$Y=y$的条件概率，$\pi_{\mathcal{X}}(x)$表示$x$的边缘概率密度。</p>
<p><strong>2. 条件概率最大化</strong></p>
<p>在给定$x \in \mathcal{X}$的条件下，Bayes最优预测$h^{\text{Bayes}}(x)$应选择能够最小化零一损失的标签。换句话说，我们选择$y$使得$\pi_{Y|X=x}(y)$最大。</p>
<p>由于$\mathcal{Y}$只有两个标签（$1$和$-1$），如果$\mathbb{P}(Y=1|X=x) &gt; 1/2$，我们选择$Y=1$；如果$\mathbb{P}(Y=-1|X=x) \geq 1/2$，我们选择$Y=-1$。</p>
<p><strong>3. 泛化误差最小化</strong></p>
<p>我们证明在这种选择下，$h^{\text{Bayes}}(x)$使得泛化误差$R(h)$最小。直观上，选取概率较大的标签可以最小化错误预测的风险，从而最小化损失。</p>
<h3 id="⛺-swig￼9">⛺<wavy>贝叶斯误差（Bayes Error）</wavy></h3>
<p><strong>Bayes误差</strong>，记作$R^{\text{Bayes}} := R(h^{\text{Bayes}})$，是基于Bayes Hypothesis计算的泛化误差。即使有最优的机器学习模型，这个误差依然存在，这是由于数据的固有不确定性导致的。这种误差被称为<strong>不可约误差</strong>（irreducible error）。</p>
<p>在二分类问题中，当使用零一损失时，最优的Bayes分类器会选择条件概率较大的标签，这可以保证错误率最小。这就是为什么机器学习算法通常追求逼近Bayes最优分类器的原因——它代表了分类问题中可以达到的最佳性能。</p>
<h3 id="⛺-swig￼10">⛺<wavy>Bias-Variance Trade-off</wavy></h3>
<p>在机器学习中，偏差-方差权衡（Bias-Variance Trade-off）是选择模型时的一个关键因素。不同的模型可能会有不同的偏差和方差，这直接影响它们在训练集和测试集上的表现。偏差和方差的平衡关系对于减少模型的泛化误差（即测试误差）非常重要。它解释了为什么我们需要在<strong>模型复杂度</strong>和<strong>模型泛化能力</strong>之间做权衡，以避免模型出现欠拟合和过拟合。</p>
<p>假设我们有一个假设$h \in \mathcal{H}$，它属于一个假设空间 $\mathcal{H}$。</p>
<h4 id="🪐-swig￼11">🪐<u>泛化误差的分解</u></h4>
<p>我们可以将某个假设 $h$ 的泛化误差 $R(h)$ 分解为三个部分：</p>
<p>$$<br>
R(h) = R^{\text{Bayes}} + \underbrace{\left( R(h) - \inf_{h’ \in \mathcal{H}} R(h’) \right)}<em>{\text{= ‘variance’}} + \underbrace{\left( \inf</em>{h’ \in \mathcal{H}} R(h’) - R^{\text{Bayes}} \right)}_{\text{= ‘bias’}}.<br>
$$</p>
<p>在这个公式中：</p>
<ol>
<li>
<p><strong>$R^{\text{Bayes}}$</strong>：这是 <strong>Bayes误差</strong>，表示了数据本身的不可约误差。即使我们有一个完美的模型，由于数据中的随机性或噪声，某些数据点可能还是无法正确分类。因此，$R^{\text{Bayes}}$ 是“问题固有的误差”。</p>
</li>
<li>
<p><strong>偏差（Bias）</strong>：$\inf_{h’ \in \mathcal{H}} R(h’) - R^{\text{Bayes}}$ 表示最优模型的误差与 Bayes误差之间的差距。偏差反映了模型的“固有误差”——即使我们在假设空间$\mathcal{H}$中找到最优模型，它的误差仍然与最理想的模型有一定距离。</p>
</li>
</ol>
<div class="note info flat"><p>这里的偏差告诉我们，假设空间 $\mathcal{H}$ 中的最优模型与 Bayes模型之间的差距。如果 $\mathcal{H}$ 太小，无法包含一个接近 Bayes假设的模型，则偏差会较大。</p>
<ul>
<li><strong>选择合适的假设空间</strong>：在选择假设空间 $\mathcal{H}$ 时，需要确保它足够小，使得模型可以有效泛化（即方差小），但也要足够大，以便包含一个接近 Bayes假设的函数（即偏差小）。</li>
<li><strong>嵌套假设空间</strong>：在实际应用中，我们经常有多个嵌套的假设空间 $\mathcal{H}_1 \subset \mathcal{H}_2 \subset \cdots$。当我们选择更大的假设空间时，偏差通常会减少，但方差可能会增加。因此，我们需要找到一个合适的假设空间，使得偏差和方差都保持在合理范围内。</li>
</ul>
</div>
<ol>
<li><strong>方差（Variance）</strong>：$R(h) - \inf_{h’ \in \mathcal{H}} R(h’)$ 表示当前模型 $h$ 与假设空间$\mathcal{H}$ 中最优模型的泛化误差之间的差异。方差反映了模型对于数据波动的敏感性。高方差通常意味着模型过于复杂，容易对训练数据过拟合，从而在测试数据上表现不佳。</li>
</ol>
<div class="note blue icon-padding simple"><i class="note-icon fas fa-bullhorn"></i><ol>
<li><strong>经验误差</strong> $\tilde{R}(h)$：通常情况下，我们通过最小化训练数据上的经验误差 $\tilde{R}(h)$ 来选择模型。这里假设 $h$ 是使 $\tilde{R}(h)$ 最小的模型。</li>
<li><strong>泛化误差界限</strong>：我们可以利用经验误差和泛化误差之间的界限来描述这个误差的大小。假设 $h$ 最小化 $\tilde{R}(h)$，那么我们有：<br>
$$<br>
-\tilde{R}(h) \leq \inf_{h’ \in \mathcal{H}} R(h’) - \tilde{R}(h) \leq R(h) - \tilde{R}(h) \leq \text{Rad}(\mathcal{H}) + \sqrt{\frac{\log(1 - \delta)}{2N}},<br>
$$<br>
其中，$\text{Rad}(\mathcal{H})$ 是假设空间的 Rademacher 复杂度，$\delta$ 是置信水平，$N$ 是样本数量。</li>
<li><strong>方差估计</strong>：通过这个不等式，我们可以进一步得到方差的上界：<br>
$$<br>
R(h) - \inf_{h’ \in \mathcal{H}} R(h’) \leq 2 \max \left{ \tilde{R}(h), \text{Rad}(\mathcal{H}) + \sqrt{\frac{\log(1 - \delta)}{2N}} \right},<br>
$$<br>
其中右边的项表示了模型方差的一个上界。</li>
</ol>
</div>
<div class="note red icon-padding flat"><i class="note-icon fas fa-fan"></i><p>综合起来，泛化误差 R(h) 由以下三部分组成：</p>
<ul>
<li>Bayes误差：这是问题固有的误差，不可避免。</li>
<li>偏差：这是模型选择导致的误差。如果模型过于简单（例如使用线性模型来拟合非线性数据），则偏差会较大。</li>
<li>方差：这是模型对数据变化的敏感性。若模型过于复杂，它对训练数据的细节也会非常敏感，从而导致在测试集上的表现不稳定。</li>
</ul>
</div>
<h4 id="🪐-swig￼15">🪐<u>偏差-方差权衡的重要性</u></h4>
<p>在模型选择时，我们需要在偏差和方差之间找到平衡：</p>
<ul>
<li>如果模型过于简单，则偏差大，但方差小，称为<strong>欠拟合</strong>。</li>
<li>如果模型过于复杂，则偏差小，但方差大，称为<strong>过拟合</strong>。</li>
</ul>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_2.png" alt="Image" style="zoom: 67%;" />
<p>一个好的模型应该适度复杂，使得偏差和方差都尽可能小，从而使得泛化误差最低。</p>
<h4 id="🪐-swig￼16">🪐<u>偏差-方差权衡的图形解释</u></h4>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_1.png" alt="Image" style="zoom: 67%;" />
<p>在上图中，目标的中心是一个完美预测正确值的模型。随着我们远离靶心，我们的预测变得越来越糟。我们可以重复我们的模型构建过程，以获得对目标的单独命中。</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_3.png" alt="Image" style="zoom: 33%;" />
<p>上面这张图很好地展示了 <strong>泛化误差 $R(\hat{h})$ 与假设空间 $\mathcal{H}_K$ 的关系</strong>，帮助我们理解偏差-方差权衡如何影响模型选择。</p>
<p>图中横轴表示假设空间的复杂度 $\mathcal{H}_K$，随着假设空间的增大，模型能够拟合更复杂的数据模式。纵轴表示模型的泛化误差 $R(\hat{h})$。当我们选择一个假设空间 $\mathcal{H}_K$ 来最小化经验误差时，泛化误差的变化通常表现为一个<strong>U型曲线</strong>：</p>
<ol>
<li>
<p><strong>左侧（高偏差区域）</strong>：</p>
<ul>
<li>在假设空间较小的情况下，模型可能过于简单，不能有效地捕捉数据的特征。这导致了<strong>高偏差（bias）</strong>，泛化误差较大。</li>
<li>此时，模型的泛化误差主要受偏差影响，因为它无法很好地拟合数据的结构，表现为<strong>欠拟合</strong>。</li>
</ul>
</li>
<li>
<p><strong>右侧（高方差区域）</strong>：</p>
<ul>
<li>随着假设空间的增大，模型可以更灵活地拟合数据。然而，假设空间过大时，模型可能会过于复杂，以至于对训练数据的噪声也进行了拟合。这导致了<strong>高方差（variance）</strong>，泛化误差再次增大。</li>
<li>此时，模型的泛化误差主要受方差影响，因为它在训练数据上表现良好，但在测试数据上表现不佳，表现为<strong>过拟合</strong>。</li>
</ul>
</li>
<li>
<p><strong>中间区域（偏差-方差权衡的最佳点）</strong>：</p>
<ul>
<li>在假设空间复杂度适中的情况下，模型的泛化误差最低，这是偏差和方差之间的最佳平衡点。</li>
<li>这里的模型既不过于简单也不过于复杂，能很好地概括数据的规律，同时对数据的波动不敏感，通常是模型选择的理想位置。</li>
</ul>
</li>
</ol>
<p>当然这张图也可以更好表现出这个关系</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_4.png" alt="Image" style="zoom:67%;" />
<h4 id="🪐-swig￼17">🪐<u>Example1</u></h4>
<p>假设：</p>
<ol>
<li><strong>输入空间</strong> $\mathcal{X}$ 和 <strong>输出空间</strong> $\mathcal{Y}$ 都是实数域 $\mathbb{R}$。</li>
<li><strong>损失函数</strong> $L$ 是平方损失函数（quadratic loss function），即 $L(y, y’) = (y - y’)^2$，用于衡量预测值与真实值之间的差异。</li>
<li><strong>字典</strong> ${\psi_1, \psi_2, \dots}$ 是一组基函数，属于 $L^2(\mathcal{X}, \mu_{\mathcal{X}}; \mathcal{Y})$ 空间（即平方可积函数空间）。这组基函数可以用来表示目标函数的近似。</li>
</ol>
<p>我们通过基函数的数量 $K$ 来构建一个包含不同模型复杂度的假设空间族。具体定义如下：</p>
<p>$$<br>
\mathcal{H}<em>K := \left{ \sum</em>{k=1}^K w_k \psi_k(\cdot) : w_1, \dots, w_K \in \mathbb{R} \right},<br>
$$</p>
<p>其中，$K \in \mathbb{N}$ 表示我们使用的基函数的数量。</p>
<span class='p red'>解释</span>
<ul>
<li><strong>假设空间族 $\mathcal{H}_K$</strong>：这里的 $\mathcal{H}_K$ 表示由 $K$ 个基函数组成的假设空间。每一个假设空间 $\mathcal{H}_K$ 包含了所有可以表示为基函数加权和的函数，权重为 $w_1, \dots, w_K$。</li>
<li><strong>模型复杂度</strong>：$K$ 的大小决定了假设空间的复杂度。$K$ 越大，假设空间 $\mathcal{H}_K$ 越复杂，可以拟合的函数种类越多；$K$ 越小，假设空间越简单。</li>
</ul>
<blockquote alt="info"><p>在实际应用中，基函数可以是不同的类型，例如多项式、傅里叶基、或小波基。对于不同的 K 值，我们可以得到一系列嵌套的假设空间，从而实现不同的模型复杂度。这与偏差-方差权衡有关：当 K 较小时，模型可能会欠拟合（高偏差）；当 K 较大时，模型可能会过拟合（高方差）。</p></blockquote>
<h2 id="🥕-swig￼19">🥕 <emp>Deep Learning(深度学习)</emp></h2>
<p>之前的学习讲的是线性分类器，具有一定的局限性，比如，某些数据分布的类别并不能通过一个超平面（即一个直线或一个平面）来分开。下图就是一个例子，展示了一组带标签的数据点（黑色点和白色圈），这些数据点的分布使得无法使用单一的超平面将两类分开。</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_5.png" alt="Image" style="zoom:50%;" />
<p>上图展示了一组位于$\mathcal{X} := \mathbb{R}^2$上的数据点，标签为 $\mathcal{Y} := {-1, 1}$。其中，黑点（$\bullet$）和白圈（$\circ$）分别代表不同的类别。由于这些数据点的分布特性，单一的超平面无法将它们分开。</p>
<p>像这样的复杂分类问题，我们需要更复杂的假设空间来实现更好的分离。<strong>深度学习</strong>是一种能够组合简单假设以构建更复杂假设空间的方法，它使用<strong>多层神经网络</strong>来实现这一点。</p>
<h3 id="⛺-swig￼20">⛺<wavy>Artificial neural networks</wavy></h3>
<p>这一部分介绍了 <strong>人工神经网络</strong> 中常用的<strong>激活函数（activation function）</strong>。激活函数是神经网络的关键组件，它们为神经元引入非线性，使得网络可以学习复杂的模式和特征，可以任意逼近任何非线性函数，这样神经网络就可以应用到众多非线性模型中。</p>
<h4 id="🪐-swig￼21">🪐<u>激活函数</u></h4>
<p>假设 $\varphi : \mathbb{R} \rightarrow \mathbb{R}$ 是一个标量函数，我们将其称为激活函数。在神经网络中，激活函数可以应用于向量的每一个分量，定义为：<br>
$$<br>
\varphi(x) := (\varphi(x_1), \dots, \varphi(x_n))^T,<br>
$$<br>
其中 $x \in \mathbb{R}^n$，$n \in \mathbb{N}$。</p>
<p><em>以下是一些常见的激活函数：</em></p>
<ol>
<li>
<p><strong>Heaviside函数</strong>（阶跃函数）：<br>
$$<br>
\varphi(x) := 1[x \geq 0] \quad (x \in \mathbb{R}),<br>
$$<br>
它是一个二值函数，在$x \geq 0$时输出1，否则输出0。</p>
</li>
<li>
<p><strong>Logistic函数</strong>（逻辑函数或S形函数）：<br>
$$<br>
\varphi(x) := \frac{1}{1 + \exp(-x)} \quad (x \in \mathbb{R}),<br>
$$<br>
它将输入压缩到 $[0, 1]$ 区间，常用于二分类模型的输出层。</p>
</li>
<li>
<p><strong>ReLU（Rectified Linear Unit）</strong>：<br>
$$<br>
\varphi(x) := \max{x, 0} = x1[x \geq 0] \quad (x \in \mathbb{R}),<br>
$$<br>
ReLU在 $x \geq 0$ 时输出$x$，否则输出0。ReLU是现代神经网络中最常用的激活函数之一，因其计算简单且缓解了梯度消失问题。</p>
</li>
<li>
<p><strong>Softplus函数</strong>：<br>
$$<br>
\varphi(x) := \log(1 + \exp(x)) \quad (x \in \mathbb{R}),<br>
$$<br>
Softplus是ReLU的光滑版本，适用于需要平滑非线性的场景。</p>
</li>
</ol>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_6.png" alt="Image" style="zoom:50%;" />
<h3 id="⛺-swig￼22">⛺<wavy>单层感知器（Single-layer Perceptron）</wavy></h3>
<p>假设我们有一个输入空间 $\mathcal{X} := \mathbb{R}^M$ 和标签空间 $\mathcal{Y} \subset \mathbb{R}^J$。单层感知器的定义如下：</p>
<p>$$<br>
H(x; (W, b)) := \varphi(Wx + b),<br>
$$</p>
<p>其中：</p>
<ul>
<li><strong>$W \in \mathbb{R}^{J \times M}$</strong>：权重矩阵，每个元素表示连接不同输入和输出之间的权重。</li>
<li><strong>$b \in \mathbb{R}^J$</strong>：偏置向量，用于调整激活函数的输出。</li>
</ul>
<p>该模型通过激活函数 $\varphi$（例如ReLU或sigmoid）对线性组合 $Wx + b$ 进行非线性变换。这个结构类似于大脑神经元的行为，每个输入信号都会被加权并传递给激活函数。</p>
<p>下图是单层感知器的示意图，它包含三个输入节点（$x_1, x_2, x_3$），两个输出节点（$y_1, y_2$），以及相应的权重和偏置。</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_7.png" alt="Image" style="zoom:50%;" />
<h3 id="⛺-swig￼23">⛺<wavy>多层感知器（Multilayer Perceptron, MLP）</wavy></h3>
<p>为了处理更复杂的问题，单层感知器通常不足以拟合复杂的非线性关系。为了解决这个问题，我们可以将多个单层感知器堆叠起来形成<strong>多层感知器</strong>，即神经网络中的<strong>前馈神经网络</strong>。其模型表示为：</p>
<p>$$<br>
H\left(x, (W^{(\ell)}, b^{(\ell)})_{l=1}^L \right) := a^{(L)},<br>
$$</p>
<p>其中：</p>
<ul>
<li><strong>$L$</strong>：网络的层数。</li>
<li><strong>$a^{(\ell)}$</strong>：第 $\ell$ 层的输出，递归定义如下：
<ul>
<li>$a^{(0)} := x$ 为输入层的输出。</li>
<li>$a^{(\ell)} := \varphi(W^{(\ell)} a^{(\ell-1)} + b^{(\ell)})$，其中 $\ell = 1, \dots, L$。</li>
</ul>
</li>
</ul>
<p>这里，<strong>$W^{(\ell)}$ 和 $b^{(\ell)}$</strong> 分别表示第 $\ell$ 层的权重矩阵和偏置向量。</p>
<p>在多层感知器中，参数空间 $\mathcal{W}$ 由所有层的权重矩阵和偏置向量组成，定义为：</p>
<p>$$<br>
\mathcal{W} := \mathbb{R}^{K(1) \times K(0)} \times \mathbb{R}^{K(1)} \times \dots \times \mathbb{R}^{K(L) \times K(L-1)} \times \mathbb{R}^{K(L)},<br>
$$</p>
<p>其中 $K(0) = M$ 是输入的维数，$K(L) = J$ 是输出的维数，其他 $K(\ell)$ 表示每层的神经元数量。</p>
<img src="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/week8_8.png" alt="Image" style="zoom:50%;" />
<p>上图展示了一个<strong>两层多层感知器（multilayer perceptron, MLP</strong>的结构。</p>
<p>在图中我们可以得到以下几点：</p>
<ol>
<li>
<p><strong>输入层</strong>：输入向量为 $\mathcal{X} = \mathbb{R}^3$，即输入有三个特征（$x_1, x_2, x_3$）。</p>
</li>
<li>
<p><strong>隐藏层</strong>：</p>
<ul>
<li>第一层隐藏层有 $K(1) = 4$ 个神经元。</li>
<li>权重矩阵 $W^{(1)}$ 的维度是 $4 \times 3$（4个输出和3个输入），属于 $\mathbb{R}^{4 \times 3}$。</li>
<li>偏置向量 $b^{(1)}$ 是一个 4 维向量，属于 $\mathbb{R}^4$。</li>
</ul>
</li>
<li>
<p><strong>输出层</strong>：</p>
<ul>
<li>输出层有 $K(2) = 2$ 个神经元，即输出为 $\mathcal{Y} = \mathbb{R}^2$。</li>
<li>权重矩阵 $W^{(2)}$ 的维度是 $2 \times 4$（2个输出和4个输入），属于 $\mathbb{R}^{2 \times 4}$。</li>
<li>偏置向量 $b^{(2)}$ 是一个 2 维向量，属于 $\mathbb{R}^2$。</li>
</ul>
</li>
</ol>
<hr>
<p>整个network的数学表示如下：</p>
<ol>
<li>
<p><strong>第一层计算</strong>：</p>
<ul>
<li>首先计算 $a^{(1)} = \varphi(W^{(1)} x + b^{(1)})$，其中 $x$ 是输入向量，$W^{(1)}$ 是第一层的权重矩阵，$b^{(1)}$ 是偏置向量，$\varphi$ 是激活函数。</li>
<li>计算结果 $a^{(1)}$ 是第一层的输出向量，它将作为下一层的输入。</li>
</ul>
</li>
<li>
<p><strong>第二层计算</strong>：</p>
<ul>
<li>第二层的输出为 $a^{(2)} = \varphi(W^{(2)} a^{(1)} + b^{(2)})$。</li>
<li>这里的 $a^{(2)}$ 就是网络的最终输出，用于预测或分类任务。</li>
</ul>
</li>
</ol>
<blockquote>
<ul>
<li><strong>隐藏层（Hidden Layers）</strong>：在多层感知器中，除了最后一层（输出层 $a^{(L)}$）以外的所有中间层都被称为隐藏层。这些层不直接与网络的输入或输出相连，而是负责处理和提取输入数据中的特征信息。</li>
<li><strong>输出层（Output Layer）</strong>：最后一层 $a^{(L)}$ 是输出层，负责输出最终的预测结果。在网络结构中，我们通常将这层直接映射为输出数据的格式。</li>
<li><strong>输入层（Input Layer）</strong>：输入层并不直接参与神经元的计算，表示为 $a^{(0)} = x$。它只是用来接收输入数据，并将其传递给第一个隐藏层。</li>
</ul>
</blockquote>
<ol>
<li><strong>神经网络的命名</strong>：
<ul>
<li><strong>单层感知器（Single-layer Perceptron）<strong>和</strong>多层感知器（Multilayer Perceptron, MLP）<strong>统称为</strong>人工神经网络（Artificial Neural Network, NN）</strong>。</li>
<li>如果一个神经网络包含多个隐藏层（即深层结构），那么这个网络也被称为<strong>深度神经网络（Deep Neural Network, DNN）</strong>。</li>
</ul>
</li>
<li><strong>神经元（Neurons）</strong>：上面两个感知器示意图中的每个矩形框代表一个神经元。神经元是网络的基本计算单元，每个神经元会接受输入、进行加权求和，然后通过激活函数生成输出。</li>
<li><strong>全连接（Fully Connected）</strong>：<strong>全连接神经网络</strong>是一种<strong>多层感知器</strong>结构。每一层的每一个节点与下一层的每一个节点相连接，并伴有运算关系。这种连接方式增加了模型的复杂度，但同时会带来计算开销。在大型神经网络中，使用全连接并不总是最优的，因为这样会导致过多的参数。</li>
</ol>
<div class="note purple icon-padding simple"><i class="note-icon far fa-hand-scissors"></i><ul>
<li>单层感知器只能构建简单的线性模型，通常用于简单的线性可分问题。</li>
<li>多层感知器通过多层结构引入复杂的非线性映射，适用于复杂的分类和回归任务。</li>
</ul>
</div>
<p>以上文档的pdf可以用以下链接下载：</p>
<a target="_blank" rel="noopener" href="https://github.com/pennyzhao1507288/img_store/raw/main/36160%20machine%20application/36160_week8.pdf" class="download-button" download>
<i class="fas fa-download"></i> 下载文档
</a>
<div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-red"  style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100"><p>观看结束，感谢观看</p></div></div>
<script src="https://giscus.app/client.js"
        data-repo="pennyzhao1507288/pennyzhao1507288.github.io"
        data-repo-id="R_kgDONMjCuQ"
        data-category="Announcements"
        data-category-id="DIC_kwDONMjCuc4CkIEb"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://pennyzhao1507288.github.io">Penny Zhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://pennyzhao1507288.github.io/posts/1deaf85c/">https://pennyzhao1507288.github.io/posts/1deaf85c/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://pennyzhao1507288.github.io" target="_blank">We are the Champions</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/math/">math</a><a class="post-meta__tags" href="/tags/machine-learning/">machine learning</a></div><div class="post-share"><div class="social-share" data-image="/cover_image/36160_week8_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/223d6b0/" title="EM algorithm and Bayes classifier"><img class="cover" src="/cover_image/38161_week8_cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">EM algorithm and Bayes classifier</div></div><div class="info-2"><div class="info-item-1"> 🏂Multivariate Statistics and Machine Learning 开始观看  🥕 EM算法 先来回顾一下log-likelihood functions for a GMM:  ⛺完全数据的对数似然函数（Complete Data Log-Likelihood） 首先，假设我们有数据 X\mathbf{X}X 和隐藏的变量 y\mathbf{y}y，则完全数据的对数似然函数定义为： ℓ(θ∣X,y)=∑i=1nlog⁡f(xi,yi∣θ)=∑i=1nlog⁡πyiNd(xi∣μyi,Σyi)\ell(\theta|\mathbf{X}, \mathbf{y}) = \sum_{i=1}^{n} \log f(x_i, y_i|\theta) = \sum_{i=1}^{n} \log \pi_{y_i} N_d(x_i|\mu_{y_i}, \Sigma_{y_i}) ℓ(θ∣X,y)=i=1∑n​logf(xi​,yi​∣θ)=i=1∑n​logπyi​​Nd​(xi​∣μyi​​,Σyi​​) 这个公式表示我们在已知每个数据点...</div></div></div></a><a class="pagination-related" href="/posts/67e70231/" title="Introduction to AI-Visual Odometry"><img class="cover" src="/AI/week7_cover2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Introduction to AI-Visual Odometry</div></div><div class="info-2"><div class="info-item-1"> Computer Vision 开始观看  Visual Odometry  什么是Visual Odometry Visual Odometry (VO) （视觉里程计）是一种技术，通过摄像头捕捉的图像来推测机器人的位置和方向。这个过程有点像我们走路时观察周围的物体来判断自己在移动的方向和距离。  为什么要用Visual Odometry？ 在自动导航中，我们希望机器人能知道自己在环境中的位置和方向。比如一辆自动驾驶汽车需要实时了解自己在马路上的位置，以避免偏离车道或撞上其他车辆。  Wheel Odometry 是通过轮子的转动来估算位置。  Wheel odometry 然而，Wheel Odometry 有很多限制，比如它容易受到地形影响（泥泞地面、打滑的地方）。 而 Visual Odometry 则利用摄像头的视觉信息，能够在这些不平稳的环境下更精确地估算位置。  Visual Odometry的工作原理 VO...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/223d6b0/" title="EM algorithm and Bayes classifier"><img class="cover" src="/cover_image/38161_week8_cover.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-11</div><div class="info-item-2">EM algorithm and Bayes classifier</div></div><div class="info-2"><div class="info-item-1"> 🏂Multivariate Statistics and Machine Learning 开始观看  🥕 EM算法 先来回顾一下log-likelihood functions for a GMM:  ⛺完全数据的对数似然函数（Complete Data Log-Likelihood） 首先，假设我们有数据 X\mathbf{X}X 和隐藏的变量 y\mathbf{y}y，则完全数据的对数似然函数定义为： ℓ(θ∣X,y)=∑i=1nlog⁡f(xi,yi∣θ)=∑i=1nlog⁡πyiNd(xi∣μyi,Σyi)\ell(\theta|\mathbf{X}, \mathbf{y}) = \sum_{i=1}^{n} \log f(x_i, y_i|\theta) = \sum_{i=1}^{n} \log \pi_{y_i} N_d(x_i|\mu_{y_i}, \Sigma_{y_i}) ℓ(θ∣X,y)=i=1∑n​logf(xi​,yi​∣θ)=i=1∑n​logπyi​​Nd​(xi​∣μyi​​,Σyi​​) 这个公式表示我们在已知每个数据点...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/profile.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Penny Zhao</div><div class="author-info-description">愿世间美好与你环环相扣</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/m0_69003698" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="mailto:zhaopeiyu150728899@163.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">你好，有需要可以关注公众号：画皮述说馆</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">🏂Machine learning application</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%A5%95-swig%EF%BF%BC2"><span class="toc-number">1.1.</span> <span class="toc-text">🥕 Bayes hypothesis and Bayes noise</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC3"><span class="toc-number">1.1.1.</span> <span class="toc-text">⛺Bayes Hypothesis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC4"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">🪐定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC5"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">🪐Bayes Hypothesis的作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC6"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">🪐特定情境下的Bayes Hypothesis</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC7"><span class="toc-number">1.1.2.</span> <span class="toc-text">⛺定理 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC9"><span class="toc-number">1.1.3.</span> <span class="toc-text">⛺贝叶斯误差（Bayes Error）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC10"><span class="toc-number">1.1.4.</span> <span class="toc-text">⛺Bias-Variance Trade-off</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC11"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">🪐泛化误差的分解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC15"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">🪐偏差-方差权衡的重要性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC16"><span class="toc-number">1.1.4.3.</span> <span class="toc-text">🪐偏差-方差权衡的图形解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC17"><span class="toc-number">1.1.4.4.</span> <span class="toc-text">🪐Example1</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%A5%95-swig%EF%BF%BC19"><span class="toc-number">1.2.</span> <span class="toc-text">🥕 Deep Learning(深度学习)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC20"><span class="toc-number">1.2.1.</span> <span class="toc-text">⛺Artificial neural networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%AA%90-swig%EF%BF%BC21"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">🪐激活函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC22"><span class="toc-number">1.2.2.</span> <span class="toc-text">⛺单层感知器（Single-layer Perceptron）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9B%BA-swig%EF%BF%BC23"><span class="toc-number">1.2.3.</span> <span class="toc-text">⛺多层感知器（Multilayer Perceptron, MLP）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/223d6b0/" title="EM algorithm and Bayes classifier"><img src="/cover_image/38161_week8_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="EM algorithm and Bayes classifier"/></a><div class="content"><a class="title" href="/posts/223d6b0/" title="EM algorithm and Bayes classifier">EM algorithm and Bayes classifier</a><time datetime="2024-11-11T03:15:30.000Z" title="发表于 2024-11-11 11:15:30">2024-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/1deaf85c/" title="bayes hypothesis and artificial neural network"><img src="/cover_image/36160_week8_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="bayes hypothesis and artificial neural network"/></a><div class="content"><a class="title" href="/posts/1deaf85c/" title="bayes hypothesis and artificial neural network">bayes hypothesis and artificial neural network</a><time datetime="2024-11-10T23:14:28.000Z" title="发表于 2024-11-11 07:14:28">2024-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/67e70231/" title="Introduction to AI-Visual Odometry"><img src="/AI/week7_cover2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Introduction to AI-Visual Odometry"/></a><div class="content"><a class="title" href="/posts/67e70231/" title="Introduction to AI-Visual Odometry">Introduction to AI-Visual Odometry</a><time datetime="2024-11-10T18:12:21.000Z" title="发表于 2024-11-11 02:12:21">2024-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8b3ac367/" title="Introduction to AI-Feature Detection and Matching"><img src="/AI/week7_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Introduction to AI-Feature Detection and Matching"/></a><div class="content"><a class="title" href="/posts/8b3ac367/" title="Introduction to AI-Feature Detection and Matching">Introduction to AI-Feature Detection and Matching</a><time datetime="2024-11-09T00:21:29.000Z" title="发表于 2024-11-09 08:21:29">2024-11-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/eec5758c/" title="Welcome To Penny's Blog"><img src="/AI/mylog_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Welcome To Penny's Blog"/></a><div class="content"><a class="title" href="/posts/eec5758c/" title="Welcome To Penny's Blog">Welcome To Penny's Blog</a><time datetime="2024-11-08T20:31:36.383Z" title="发表于 2024-11-09 04:31:36">2024-11-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Penny Zhao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'pennyzhao1507288/pennyzhao1507288.github.io',
      'data-repo-id': 'R_kgDONMjCuQ',
      'data-category-id': 'DIC_kwDONMjCuc4CkIEb',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !true) {
    if (true) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>